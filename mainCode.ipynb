{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizeData(X_train, X_test):\n",
    "    cv = CountVectorizer(stop_words=stopwords.words('english'))\n",
    "    X_train_counts = cv.fit_transform(X_train['headline'])\n",
    "    X_test_counts = cv.transform(X_test['headline'])\n",
    "    return X_train_counts, X_test_counts\n",
    "\n",
    "\n",
    "def splitTrainingData(df, featureCols, targetCols, random=False):\n",
    "    state = 42 if random else None\n",
    "    X = df[featureCols]\n",
    "    y = df[targetCols]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=state)\n",
    "    X_train, X_test = vectorizeData(X_train, X_test)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Read in the data, we are only interested in headlines and category. \n",
    "One hot encode the categories\n",
    "TODO: More efficient way?\n",
    "\"\"\"\n",
    "df = pd.read_json(\"data/News_Category_Dataset_v3.json\", lines=True)\n",
    "df = df[['headline', 'category']]\n",
    "# print(df['category'].unique())\n",
    "# get only subset of categories\n",
    "# df = df[df['category'].isin(['POLITICS', 'ENTERTAINMENT', 'TECH'])]\n",
    "df = pd.get_dummies(df, columns=['category'])\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split the data into training and testing\n",
    "\"\"\"\n",
    "feature_columns = ['headline']\n",
    "category_columns = df.columns[1:]\n",
    "df = df.sample(frac=.1).reset_index(drop=True)\n",
    "X_train, X_test, y_train, y_test = splitTrainingData(df, feature_columns, category_columns)\n",
    "X_train = X_train.toarray()\n",
    "X_test = X_test.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Declare the Model \"\"\"\n",
    "from keras.layers import Dense\n",
    "from keras.initializers import HeNormal\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Dropout\n",
    "from keras.losses import CategoricalCrossentropy \n",
    "from keras.layers import MultiHeadAttention, Input \n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class NLTK_Classifier:\n",
    "    def __init__(self, input_shape=None, num_classes=3):\n",
    "        inputs = Input(shape=input_shape)\n",
    "        transformer_output = self.transformLayer(heads=8)(inputs, inputs, inputs)\n",
    "        dense_output = Dense(4096, activation='relu')(transformer_output)\n",
    "        outputs = Dense(num_classes, activation='softmax')(dense_output)\n",
    "\n",
    "        self.model = Model(inputs=inputs, outputs=outputs)\n",
    "        \n",
    "    # custom transformer layer \n",
    "    \"\"\"\n",
    "    A transformer layer expects inputs to be of shape (batch_size, seq_len, dimension of each input vector)\n",
    "    \"\"\"\n",
    "    def transformLayer(self, heads):\n",
    "        return MultiHeadAttention(\n",
    "                num_heads=heads, key_dim=8, dropout=0.1,\n",
    "                kernel_initializer='he_normal', bias_initializer='zeros',\n",
    "                bias_regularizer=l2(0.01), kernel_regularizer=l2(0.01)\n",
    "        )\n",
    "    \n",
    "    # Customer Dense layer\n",
    "    def DenseLayer(self, nodes, activation='relu'):\n",
    "        return Dense(\n",
    "            nodes, activation=activation, \n",
    "            kernel_initializer=HeNormal(), bias_initializer=HeNormal(),\n",
    "            kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)\n",
    "        )\n",
    "\n",
    "    # Resets weights to HeNormal\n",
    "    def reset_weights(self):\n",
    "        initial_weights = self.model.get_weights()\n",
    "        self.model.set_weights(initial_weights)\n",
    "\n",
    "    # compile the model\n",
    "    def compile(self):\n",
    "        self.model.compile(optimizer=Adam(), loss=CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "    # Run the model. Forward fit using a learning rate scheduler\n",
    "    def fit(self, X_train, training_labels, epochs=1, batch_size=32):\n",
    "        lr_scheduler = ExponentialDecay(initial_learning_rate=0.001, decay_steps=5, decay_rate=0.5)\n",
    "        self.model.fit(X_train, training_labels, epochs=epochs, \n",
    "                    batch_size=batch_size, callbacks=[LearningRateScheduler(lr_scheduler)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows indicate headlines, columns indicate words\n",
    "print(X_train.shape)\n",
    "\n",
    "# A transformer layer expects inputs to be of shape (batch_size, seq_len, dimension of each input vector)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "print(X_train.shape)\n",
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "y_train = np.expand_dims(y_train, axis=1)\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "\n",
    "model = NLTK_Classifier(input_shape=input_shape, num_classes=y_train.shape[2])\n",
    "model.compile()\n",
    "model.reset_weights()\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=21, batch_size=64)\n",
    "\n",
    "# Save the model\n",
    "model.model.save('models/transformer_model.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the Model and test it with some data\n",
    "\"\"\"\n",
    "from keras.models import load_model\n",
    "model = load_model('models/transformer_model.keras')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
